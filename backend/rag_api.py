# rag_api.py
import uvicorn
import os
from fastapi import FastAPI
from pydantic import BaseModel
from dotenv import load_dotenv  # â­ï¸ .env íŒŒì¼ ë¡œë“œìš©

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ---------------------------------------------------------------
# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
# ---------------------------------------------------------------
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

if "OPENAI_API_KEY" not in os.environ:
    print("âš ï¸ [ê²½ê³ ] OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ---------------------------------------------------------------
# 2. FastAPI ì•± ì´ˆê¸°í™”
# ---------------------------------------------------------------
app = FastAPI(
    title="Recycle RAG API",
    description="FAISS ë²¡í„° DB ê¸°ë°˜ì˜ ì¬í™œìš© ê°€ì´ë“œ ì±—ë´‡ API",
    version="1.0.0"
)

# ---------------------------------------------------------------
# 3. ë°ì´í„° ëª¨ë¸ ì •ì˜
# ---------------------------------------------------------------
class RagQuery(BaseModel):
    user_input: str | None = None  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ (ì˜µì…˜)
    image_class: str | None = None # YOLOê°€ íƒì§€í•œ ì´ë¯¸ì§€ í´ë˜ìŠ¤ëª… (ì˜µì…˜)

class RagResponse(BaseModel):
    response_text: str

# ---------------------------------------------------------------
# 4. ì „ì—­ ë³€ìˆ˜ (ëª¨ë¸, ì²´ì¸)
# ---------------------------------------------------------------
qa_chain = None

# ---------------------------------------------------------------
# 5. ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ (Startup Event)
# ---------------------------------------------------------------
@app.on_event("startup")
def load_models():
    global qa_chain
    print("ğŸš€ [Startup] RAG API ì„œë²„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    
    try:
        # 1) LLM ëª¨ë¸ ì„¤ì • (gpt-3.5-turbo ë˜ëŠ” gpt-4o-mini ì¶”ì²œ)
        # temperature=0 : ì°½ì˜ì„±ì„ ì£½ì´ê³  ì‚¬ì‹¤(Fact) ìœ„ì£¼ë¡œ ë‹µë³€í•˜ê²Œ í•¨
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
        
        # 2) ì„ë² ë”© ëª¨ë¸
        embeddings = OpenAIEmbeddings()

        # 3) FAISS ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        vector_db_path = "my_faiss_index" # indexing.pyì—ì„œ ì €ì¥í•œ í´ë”ëª…
        if not os.path.exists(vector_db_path):
            raise FileNotFoundError(f"'{vector_db_path}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. indexing.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

        vector_store = FAISS.load_local(
            folder_path=vector_db_path, 
            embeddings=embeddings, 
            allow_dangerous_deserialization=True
        )
        
        # 4) ë¦¬íŠ¸ë¦¬ë²„ (ê²€ìƒ‰ê¸°) ì„¤ì •
        # k=3: ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ 3ê°œë¥¼ ê°€ì ¸ì˜´
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})

        # 5) â­ï¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê°€ì¥ ì¤‘ìš”í•œ ìˆ˜ì • ë¶€ë¶„) â­ï¸
        # - AIì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ê³ , ë¬¸ë§¥ ì™¸ ì •ë³´ ì‚¬ìš© ê¸ˆì§€ë¥¼ ëª…ì‹œ
        # - ê°€ë…ì„±ì„ ìœ„í•œ í¬ë§·íŒ… ì§€ì‹œ
        prompt_template = """
        ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê¼¼ê¼¼í•œ 'ì¬í™œìš© ë¶„ë¦¬ë°°ì¶œ ë„ìš°ë¯¸'ì…ë‹ˆë‹¤.
        ì•„ë˜ [ì œê³µëœ ì •ë³´(Context)]ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ [ì§ˆë¬¸(Question)]ì— ë‹µë³€í•˜ì„¸ìš”.

        [ì§€ì¹¨]
        1. **ì—„ê²©í•œ ì‚¬ì‹¤ ê¸°ë°˜**: [ì œê³µëœ ì •ë³´]ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ "ì£„ì†¡í•˜ì§€ë§Œ ì œê³µëœ ìë£Œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ë‚˜ì™€ìˆì§€ ì•Šì•„ìš”."ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.
        2. **ì¹œì ˆí•œ í†¤**: ë‹µë³€ì€ ì¡´ëŒ“ë§ë¡œ, ë¶€ë“œëŸ½ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        3. **ê°€ë…ì„±**: 
           - ë‹µë³€ì´ ê¸¸ì–´ì§€ë©´ ì¤„ë°”ê¿ˆì„ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”.
           - í•µì‹¬ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œí•˜ì„¸ìš”.
           - í•„ìš”í•˜ë‹¤ë©´ ë²ˆí˜¸ ë§¤ê¸°ê¸°(1., 2.)ë‚˜ ë¶ˆë¦¿ í¬ì¸íŠ¸(-)ë¥¼ ì‚¬ìš©í•´ ì •ë¦¬í•˜ì„¸ìš”.
           - ì ì ˆí•œ ê³³ì— ì´ëª¨ì§€(ğŸŒ±, â™»ï¸, ğŸ—‘ï¸ ë“±)ë¥¼ ì‚¬ìš©í•´ ë‹µë³€ì„ ìƒë™ê° ìˆê²Œ ë§Œë“œì„¸ìš”.

        [ì œê³µëœ ì •ë³´(Context)]
        {context}

        [ì§ˆë¬¸(Question)]
        {question}

        [ë‹µë³€]
        """
        
        PROMPT = PromptTemplate(
            template=prompt_template, 
            input_variables=["context", "question"]
        )

        # 6) RAG ì²´ì¸ ì—°ê²° (LCEL)
        qa_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | PROMPT
            | llm
            | StrOutputParser()
        )
        
        print("âœ… LLM, ì„ë² ë”©, ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ.")
        print("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ.")

    except Exception as e:
        print(f"âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        qa_chain = None


# ---------------------------------------------------------------
# 6. API ì—”ë“œí¬ì¸íŠ¸
# ---------------------------------------------------------------
@app.post("/api/rag_query", response_model=RagResponse)
async def process_rag_query(query: RagQuery):
    global qa_chain
    
    # 1. ì„œë²„ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë°©ì–´ ì½”ë“œ
    if not qa_chain:
        return RagResponse(response_text="ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨)")

    # 2. ì§ˆë¬¸ ì¡°í•© ë¡œì§ (YOLO í´ë˜ìŠ¤ + ì‚¬ìš©ì ì…ë ¥)
    input_text = query.user_input.strip() if query.user_input else ""
    detected_class = query.image_class.strip() if query.image_class else ""
    
    final_question = ""

    if detected_class and input_text:
        # ì˜ˆ: ì‚¬ì§„ì€ 'pet_bottle'ì´ê³  ì§ˆë¬¸ì€ "ëšœê»‘ì€ ì–´ë–»ê²Œ í•´?"
        final_question = f"ì‚¬ì§„ì—ì„œ ê°ì§€ëœ ë¬¼ì²´ëŠ” '{detected_class}'ì…ë‹ˆë‹¤. ì´ ë¬¼ì²´ì™€ ê´€ë ¨í•˜ì—¬ ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤: '{input_text}'. ì´ ë¬¼ì²´ì˜ ì˜¬ë°”ë¥¸ ë¶„ë¦¬ë°°ì¶œ ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    elif detected_class:
        # ì˜ˆ: ì‚¬ì§„ë§Œ ìˆê³  ì§ˆë¬¸ì€ ì—†ìŒ
        final_question = f"ì‚¬ì§„ì—ì„œ '{detected_class}'(ì´)ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ê²ƒì˜ ì˜¬ë°”ë¥¸ ë¶„ë¦¬ë°°ì¶œ ë°©ë²•ì„ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”."
    elif input_text:
        # ì˜ˆ: ì‚¬ì§„ ì—†ì´ í…ìŠ¤íŠ¸ ì§ˆë¬¸ë§Œ ìˆìŒ
        final_question = input_text
    else:
        return RagResponse(response_text="ì§ˆë¬¸í•  ë‚´ìš©ì´ë‚˜ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 3. ì²´ì¸ ì‹¤í–‰ ë° ë‹µë³€ ìƒì„±
    try:
        result = qa_chain.invoke(final_question)
        return RagResponse(response_text=result)

    except Exception as e:
        print(f"âŒ RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return RagResponse(response_text="ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë„ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

# ì§ì ‘ ì‹¤í–‰ ì‹œ ì‚¬ìš© (ê°œë°œìš©)
if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8001)